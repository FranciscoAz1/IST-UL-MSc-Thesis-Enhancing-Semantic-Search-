\chapter{Methodologies}
\label{chapter:Methodologies}
This chapter introduces the methodologies employed in this project. The focus will be on information extraction techniques such as OCR engines and layout models, enabling the capture of both textual content and document structure. The extracted information will be stored in vector databases to facilitate efficient organization and retrieval. The Retrieval-Augmented Generation (RAG) framework will be utilized to retrieve relevant information, employing prompt engineering techniques to enhance the consistency and accuracy of user interactions. RAG will be tested in conjunction with the vector database, evaluating the accuracy of the indexed documents. Models will also be tested for their summarization capabilities, ensuring that large documents can be indexed efficiently while only presenting the most relevant sections during retrieval.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Figures/graph_methods.png}
    \caption{Diagram of the project}
    \label{fig:methodology}
\end{figure}

\section{Data Collection}
The data used for testing in this thesis comprises a variety of formats commonly encountered in organizational settings, (for e.g. PDF documents, scanned images, emails and PowerPoint presentations)

These formats pose challenges due to their semi-structured or unstructured nature. The testing data is collected to simulate real-world corporate environments, ensuring the robustness of the proposed system.

The research will focus on analyzing the challenges posed by semi-structured and unstructured information, commonly encountered in organizational settings, such as PDF documents, scanned images, emails, and PowerPoint presentations.  Research to identify patterns in these documents will be conducted. As well as techniques such as OCR engines \ref{sec:ocr} and layout analysis \ref{sec:layoutparser} on different file formats.
A dataset will be compiled, with a vision to simulate real-world corporate environments to test the robustness of the system under diverse corporate scenarios.

\section{Information extraction and Structuring}
Research will focus on evaluating multimodal models for their ability to capture both textual content and document structure. Models to be studied include:
\begin{itemize}
    \item \textbf{PrimaLayout}: Identifying and classifying regions such as text, tables, and images.
    \item \textbf{PubLayNet}: Segmenting documents into logical blocks.
    \item \textbf{Nougat}: An OCR engine that converts documents into markdown, preserving the hierarchy of chapters, sections, and subsections.
    
\end{itemize}
Testing and integrating the best model to extract and preserve document structure will be studied. Aiming for the structuring of the extracted content for efficient storage and retrieval.
\section{Vector Creation and Storage}
Research with the focus on optimizing the process of generating and storing document embeddings to ensure accurate and fast retrieval. For example finding what embedding models provide the best performance for retrieval, or how can embeddings be stored and queried efficiently at scale.
Tools such as Weaviate will be used to generate vector embeddings. And store in an optimal vector database for retrieval even at scale.

\section{Prompt Engineering}
Integrate a light \ac{LLM} to run locally and serve as the interface for retrieving information from documents, with their embeddings stored in the vector database. Implement techniques to ensure that the user's prompt becomes more consistent before reaching the retrieval stage. Techniques to be investigated include dynamic query refinement, which involves automatically enriching the user prompt by adding missing context, such as prior interactions or user-specific information. Another technique is follow-up questioning, where insufficient context in the user prompt is detected, and clarifying questions are asked to refine the query and provide better retrieval context. For example:
\begin{itemize}
    \item The retrieved documents span multiple categories. Are you specifically interested in category A or category B?
    \item Would like the retrieved documents to focus on a specific time frame or region?
\end{itemize}

\section{Access Control and Security}
Investigate methodologies for access management and stored information security in the context of retrieval systems. Researching encryption and secure storage techniques for embeddings and documents. Study methods to implement user-based access restrictions to ensure privacy and compliance with organizational policies.
Then implement access control mechanisms to restrict users to authorized information. Integrate access management features to make users restriction to information dynamic.

\section{Decentralized Database}
For easier interoperability and integration, the storage for embeddings, generated and document metadata. Which are created from the pre-processing \ac{RAG} pipeline. A type of semantic database is necessary, to enhance semantic search through the internet security. For this a simple server implementation awating for semantic search requests will be implemented.

\section{Semantic Search}
With the use of embedding models. We process the prompt with an llm, for generalizing the prompt for a better search query. Then the embedding of the processed query is generated using the same embedding model used for processing the documents. Then a vector search is done to the vector database. Which finds the most relevant chunks. THe chunks are indexed to the document. The best documents are indexed.
\chapter{Planning}

To achieve the goals outlined above, including chapter \ref{chapter:Planned Results}, the project will be divided into distinct phases \ref{fig:planning}. Each phase will focus on specific milestones to ensure the timely and systematic development of the solution.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/planning.png}
    \caption{Simplified Planned Workflow}
    \label{fig:planning}
\end{figure}
\section{Until Now - Initial Research and Pre-processing of Information}
Research has been conducted on \acr{RAG}, \ac{LLM}, and \ac{VD}. Tools like Weaviate have been identified to facilitate the integration of \acr{RAG} and \ac{VD}. By creating collections that combine these techniques, various approaches can be tested and evaluated. Additionally, lightweight \ac{LLM}s such as Llama3.2, which can run fully locally, have been explored. However, considering the rapid advancements in this field, newer lightweight \ac{LLM}s with improved integration capabilities may become available in the future.

A preprocessing pipeline has been developed to extract text from documents while preserving their structure. Several OCR engines, including Google Vision, Tesseract, and Nougat, have been tested. Pre-trained layout models such as PrimaLayout and PubLayNet have also been evaluated. A script was implemented to extract text from PDFs by converting them to PNG format, enabling text extraction using Nougat \cite{blecher2023nougatneuralopticalunderstanding}.

\section{By Month 3 - Initial Framework}

The initial framework will involve creating a collection using Weaviate. Documents will be stored as vector embeddings, enabling vector search techniques such as semantic search and question-answer extraction. This collection will serve as the foundation for testing and experimentation.

\section{By Month 4 - Dataset Creation / Benchmark Implementation}
\label{sec:datasetcreation}
A dedicated dataset will be developed to ensure consistent testing across different techniques for document storage and extraction. AS stated in subsection \ref{subsec:externalbenchmark}, if a recognized external benchmark dataset  for testing vector databases is identified, it will be adopted. Otherwise, a custom dataset will be created with sufficient variety and scale to validate the system, as explained above in chapter \ref{chapter:Planned Results}.

\section{By Month 6 - Testing / Comparative Analysis}

The testing phase will focus on evaluating various techniques for document storage and extraction. \ac{LLM}s will be integrated during this process, as they may enhance certain methodologies. Inspired by prior research such as \cite{densepassageretrievalopendomainkarpukhin2020}, this project aims to achieve improved outcomes using modern, larger-scale models with better training.

Once testing is completed, the best results will be documented as a comparative analysis. Then an \ac{LLM} will be integrated to summarize the output, enabling a concise presentation of the findings.

\section{By Month 7 - Security Measures}

The program will be enhanced with robust security features to ensure controlled access to information. User roles will be defined, including standard users and administrators, where administrators will have the authority to manage access of different users to various types of documents. The program will only be accessible through authenticated user or admin accounts.

Documents will be categorized with tags such as recruitment, human resources, finances, or treasury. Access permissions will be assigned based on user roles, ensuring that each user only has access to the documents relevant to their responsibilities.

An Access Control List (ACL) will be implemented to enforce these restrictions on documents indexed in the \ac{VD}. Additionally, encryption mechanisms will be explored to protect sensitive information further. These security enhancements will be developed in collaboration with LinkConsulting, a company specializing in AI and information storage solutions like EdockLink, with proven expertise in addressing corporate software security requirements.



