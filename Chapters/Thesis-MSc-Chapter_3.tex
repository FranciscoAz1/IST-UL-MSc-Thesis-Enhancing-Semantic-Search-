% #############################################################################
% This is Chapter 3
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{State of the Art}
\label{chap:State_of_Art}
\cleardoublepage

This chapter surveys \gls{AAI}, language-model driven agents that plan, use tools, and manage memory, to improve retrieval and answer quality. We focus on methods that (i) reason and decompose tasks, (ii) call external tools and data sources, and (iii) maintain working åand long-term memory. We also connect these capabilities to \gls{RAG} pipelines that must respect organizational structure (folders, metadata) and access control.

\section{From Prompting to Agentic Behavior}
\textbf{Prompt engineering.} Well-designed instructions can control output format, tone, and constraints, and can inject short domain context. History (dialog state) may be summarized and re-provided as context; however, prompting alone does not guarantee factuality or stable tool use.

\textbf{In-context learning vs.\ fine-tuning.} Few-shot prompts adapt behavior without training, at the cost of input tokens and brittle generalization. Fine-tuning bakes patterns into model weights, reducing prompt length and latency, but requires curated data, cost, and re-deployment.

\section{Reasoning and Planning}
\subsection{Chain of Thought (CoT)}
CoT prompting was introduced by Wei et al.~\cite{chainofthought} to improve reasoning tasks where the input–output mapping is non-trivial (e.g., math problem $\to$ numerical answer). The method adds intermediate reasoning steps in natural language, which bridge the gap between question and solution.
In benchmarks such as GSM8K and MultiArith, CoT significantly increased accuracy when compared to direct prompting. A refinement, \emph{Self-Consistency CoT} (CoT-SC), samples multiple reasoning chains and returns the most consistent answer, achieving further gains (e.g., PaLM 540B improved from 55\% $\to$ 74\% on GSM8K). However, CoT remains linear: each chain is generated without exploring alternatives.

\subsubsection{Few-Shot Chain-of-Thought \cite{chainofthought}}
Handcraft 8 examples with these fields:

\begin{lstlisting}[language={},basicstyle=\ttfamily\footnotesize,caption={Few-Shot Chain-of-Thought Prompt Template}]
Q: [Your question]\\
A: [Step-by-step reasoning]. The answer is [final answer].
\end{lstlisting}

Then use this for few-shot prompting. This is what made the paper achieve significantly increased accuracy.

\subsubsection{Zero-Shot Chain-of-Thought ("Let's think step by step") \cite{chainofzero}}
\begin{lstlisting}[language={},basicstyle=\ttfamily\footnotesize,caption={Zero-Shot Chain-of-Thought Prompt Template}]
Q: [Your question here].\\
Let's think step by step.
\end{lstlisting}

The paper \cite{chainofzero} tested this prompts on the MultiArith dataset with text-davinci-002. Such prompts were such as "Let's think about this logically" , " Before we dive into the answer", "Abrakadabra!"
The template "Let's think step by step" got the highest score 78,7 \% , while others such as "Abrakadabra!" got a 15,5 \% which surprisingly isn't far from raw prompt template which scored 15,5 \%.

\subsubsection{Conclusion}
Form the paper \cite{chainofzero}, the chain of thought proposed in \cite{chainofthought} was tested to evaluate the actual difference of using few-shot prompt with chain of thought.
Where the valuable mentions from the GSM8K benchmark, using PALM (540B) model \ref{tab:chainofzero}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|cc}
        \hline
        Method & GSM8K\\
        \hline
        Zero-Shot & 12,5 \\
        \hline
        Zero-Shot-CoT \cite{chainofzero} & 43,0 \\
        \hline
        Few-Shot-CoT \cite{chainofthought} & 56,9 \\
        \hline
    \end{tabular}
    \caption{Experiment curated results using PALM (540B) \gls{LLM} model \cite{chainofzero}}
    \label{tab:chainofzero}
\end{table}

According to the results see in the table above \ref{tab:chainofzero} in all the Few-Shot-CoT is still better. But the fact that Zero-ShoT-CoT is much easier to implement withouth having to come up with specific domain CoT examples. It is the best solution for the thesis for universal application.


\subsection{Tree-of-Thoughts (ToT)}
Yao et al.~\cite{treeofthought} extended CoT by introducing branching reasoning. Each intermediate step (a ``thought'') forms part of a search tree, where multiple candidate continuations can be generated, evaluated, and backtracked. This enables deliberate problem solving under uncertainty. On tasks such as the Game of 24, Mini Crosswords, and constrained story generation, ToT substantially outperformed both standard prompting and CoT (e.g., GPT-4 with ToT achieved 74\% accuracy on Game of 24, versus 19\% with CoT). The main trade-off is computational cost: ToT requires generating and evaluating many reasoning branches, making it slower and more resource-intensive.

\subsection{Reasoning \emph{and} Acting (ReAct)}
Yao et al.~\cite{react} proposed ReAct, a framework that interleaves reasoning traces with external actions (e.g., database queries, web search, or calculator calls) and observations. Unlike CoT, which is limited to internal reasoning, ReAct closes the loop between planning and evidence gathering, allowing the model to ground its reasoning in retrieved information. On knowledge-intensive QA tasks such as HotpotQA and FEVER, ReAct improved factual accuracy and significantly reduced hallucinations (e.g., from 14\% to 6\%). It also achieved strong performance on interactive environments like ALFWorld, where reasoning must be combined with sequential decision-making. The main trade-off is higher latency and token cost, since each step requires both reasoning tokens and tool interactions, but the gains in groundedness and reliability are substantial for retrieval-augmented applications.

\subsubsection{Example}
ReAct interleaves reasoning traces with external actions and observations. The framework follows this pattern:

\begin{lstlisting}[language={},basicstyle=\ttfamily\footnotesize,caption={ReAct Framework Pattern}]
Thought: [Reasoning about what to do next]\\
Action: [Specific action to take, e.g., search, calculate, query]\\
Observation: [Result of the action]\\
Thought: [Next reasoning step based on observation]\\
...
\end{lstlisting}

For example, to answer "What is the population of Paris and how does it compare to London?":

\begin{lstlisting}[language={},breaklines=true,basicstyle=\ttfamily\footnotesize,caption={ReAct Example: Population Comparison Query}]
Q: What is the population of Paris and how does it compare to London?
Thought: I need to find the current population of both cities to make a comparison. I'll start by 
searching for Paris's population.
Action: Search[Paris population 2023]
Observation: Paris has approximately 2.16 million inhabitants in the city proper (2023 estimate).
Thought: Now I need London's population for comparison.
Action: Search[London population 2023]
Observation: London has approximately 8.98 million inhabitants (2023 estimate).
Thought: Paris has about 2.16 million people while London has 8.98 million, so London is roughly 4 
times larger.
Action: Calculate[8.98 / 2.16]
Observation: 8.98 $\div$ 2.16 $\approx$ 4.16
Thought: The calculation confirms London is about 4.16 times larger than Paris.
Final Answer: Paris has approximately 2.16 million inhabitants, while London has 8.98 million, 
making London about 4.16 times larger than Paris.
\end{lstlisting}

This interleaving of reasoning (Thought) with external actions (Action) and observations allows ReAct to ground its responses in retrieved information rather than relying solely on parametric knowledge (stored in model weights), significantly reducing hallucinations while improving factual accuracy.
\section{Tool Use and Orchestration}

While ReAct already integrates tool use within its action step, this section focuses on how such capabilities can be made more accurate.  
Tool-use frameworks aim to improve how language models discover, select, and call external tools, reducing hallucinations and execution errors.  

For example, consider a WebCommerce chatbot that interfaces with an e-commerce database, allowing customers to place new orders, check delivery status, or request refunds directly through conversation.
Each of these actions depends on precise tool calls to backend APIs—such as \texttt{createOrder()}, \texttt{getOrderStatus()}, or \texttt{updatePayment()}—where incorrect parameters or schema mismatches could lead to failed transactions or inconsistent records, emphasizing the need for accurate and validated orchestration. 

In such a setting, tool calls must be precise. Incorrect or misunderstood parameters can result in failed transactions or inconsistent order data—highlighting the importance of reliable tool use and orchestration.

\textbf{Toolformer}~\cite{schick2023toolformer} introduced the concept of \emph{self-taught tool use}.  
It fine-tunes a language model (based on GPT-J) using a self-annotation loop: during training, the model is prompted to insert possible API calls into text, these calls are executed, and their returned outputs are evaluated by measuring whether they reduce the model’s prediction loss on masked tokens.  
If an API’s result helps the model predict the next token more accurately, that example is kept as supervision.  
This process allows the model to learn when a tool should be invoked and how to integrate its output into natural language responses.  

Although Toolformer improved factual accuracy by roughly 60\% on arithmetic and translation tasks.

\textbf{Gorilla}~\cite{patil2023gorilla} advanced this idea with \emph{Retrieval-Aware Training (RAT)}.  
Instead of memorizing tool usage, Gorilla embeds both API documentation (function signatures, parameter types, examples) and natural-language queries into a joint vector space using \emph{contrastive learning}. A method that pushes matching pairs (query $\leftrightarrow$ correct API) closer together while pushing unrelated pairs apart.  
At inference time, the model retrieves the most semantically similar API specification and conditions its text generation on that retrieved context, producing syntactically valid function calls through templated generation constrained by the retrieved schema.  
On the \textit{APIBench} benchmark, Gorilla achieved \textbf{90–92\% API-call correctness} and reduced hallucinated calls by over \textbf{40\%} compared to Toolformer.  
However, like Toolformer,  it still requires re-training when entirely new APIs are introduced, limiting adaptability.

\textbf{HuggingGPT}~\cite{shen2023hugginggpt} reframed orchestration as \emph{model routing}.  
It treats the LLM as a planner that decomposes complex user requests into subtasks and, for each subtask, queries a catalog of expert models hosted on the Hugging Face hub.  
Although this demonstrated cross-modal reasoning across text, vision, and audio tasks, the approach introduced limited methodological novelty beyond Gorilla’s retrieval step.  
Its evaluation was conducted through human assessment on 130 user requests, where the best-performing model (GPT-3.5) achieved a model-selection passing rate of \textbf{93.89\%}, a rationality score of \textbf{84.29\%}, and a final-response success rate of \textbf{63\%}.



\textbf{AutoGen}~\cite{wu2023autogen} generalized orchestration into a \emph{multi-agent dialogue framework}.  
In this paradigm, multiple LLMs and tools interact conversationally, exchanging natural-language messages that represent plans, results, or verifications.  
Each agent can reason (\emph{Chain-of-Thought}), act (\emph{ReAct}), or delegate subtasks to others, enabling cooperative problem-solving.  
Benchmarks on reasoning environments such as \textit{ALFWorld} and \textit{HotpotQA} reported up to \textbf{+17\% higher task success rates} than single-agent baselines, confirming that collaboration and verification loops improve reliability.  
Nonetheless, AutoGen is computationally expensive and often unstable: extended conversation loops between agents can lead to significant inefficiency or even infinite exchanges without task resolution.  
Moreover, it lacks a standardized interface for connecting to external systems, making integration with enterprise infrastructures or databases cumbersome.

\textbf{Model Context Protocol (MCP)} introduces a standardized framework for connecting LLMs with external systems through explicit, typed interfaces.  
Unlike previous approaches that relied on learned behavior or conversational coordination, MCP formalizes tool interaction by exposing capabilities through structured \texttt{tools}, \texttt{resources}, and \texttt{prompts}.  
Each is defined by a JSON schema that specifies valid inputs and outputs, allowing clients to discover available operations and invoke them via JSON-RPC calls.  
This replaces informal prompt-based conventions with deterministic, schema-validated communication, ensuring consistency and interoperability across agents and environments.  
The following section describes MCP’s architecture in detail and how it was applied in this thesis to integrate Weaviate with LLM agents.


\section{Model Context Protocol (MCP)}

The \textbf{Model Context Protocol (MCP)} is an open standard introduced by Anthropic to connect large language models with external systems in a modular and reproducible way. It provides a universal interface---often described as a ``USB-C port for AI applications''---to integrate tools, data sources, and prompt templates without ad-hoc engineering \cite{mcp-intro}.

\subsection{Architecture} 
MCP follows a \textit{client--server model}. The \textit{host} (AI application) communicates through an MCP \textit{client} with one or more \textit{servers} that expose capabilities. Communication is structured in two layers:
(i) \textbf{Data layer:} uses JSON-RPC 2.0 to define primitives, discovery mechanisms, and lifecycle management;
(ii) \textbf{Transport layer:} handles message delivery (e.g., stdio for local, HTTP/SSE for remote) and authentication \cite{mcp-architecture}.


\subsection{Core Primitives} 
\begin{itemize}
  \item \textbf{Tools} --- typed, executable functions (e.g., search, API calls, database queries).
  \item \textbf{Resources} --- contextual data the model can access (files, records, metadata).
  \item \textbf{Prompts} --- versioned templates or workflows to standardize interactions.
  \item \textbf{Memory} --- structured persistence of state, supporting short-term caches and long-term profiles.
\end{itemize}

MCP contributes to the shift toward \textit{agentic LLMs}, where models not only generate text but also plan, act, and learn through external capabilities.  
It has rapidly become a key standard for connecting AI applications to external systems, already implemented as an MCP client in platforms such as Docker, Claude Desktop, and VS Code, and supported by multiple servers including MongoDB, Google Workspace, and LangChain.  
Additional open-source implementations include the \texttt{docker/mcp-toolkit}, \texttt{anthropic/mcp}, and \texttt{mongodb-js/mcp-server}.  
One of these servers—the \textbf{Weaviate MCP server}—was developed as part of this thesis to expose Weaviate’s schema and query interface as callable MCP \texttt{tools}.  

MCP’s main advantages are its \textbf{standardization}, by providing a unified protocol across heterogeneous backends; \textbf{reproducibility}, through versioned prompts and schema-defined tools; \textbf{extensibility}, since new capabilities can be added without architectural changes. \cite{mcp-spec}.  
In this thesis, MCP serves as the interoperability layer between Weaviate and LLM agents, enabling schema-aware retrieval and controlled reasoning over enterprise information in a reproducible and standardized way.
