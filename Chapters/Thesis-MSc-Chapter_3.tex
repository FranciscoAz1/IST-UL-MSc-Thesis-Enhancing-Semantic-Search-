% #############################################################################
% This is Chapter 3
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{State of the Art}
\label{chap:sota-agentic-ai}
\cleardoublepage

This chapter surveys \ac{AAI}, language-model driven agents that plan, use tools, and manage memory, to improve retrieval and answer quality. We focus on methods that (i) reason and decompose tasks, (ii) call external tools and data sources, and (iii) maintain working åand long-term memory. We also connect these capabilities to \ac{RAG} pipelines that must respect organizational structure (folders, metadata) and access control.

\section{From Prompting to Agentic Behavior}
\textbf{Prompt engineering.} Well-designed instructions can control output format, tone, and constraints, and can inject short domain context. History (dialog state) may be summarized and re-provided as context; however, prompting alone does not guarantee factuality or stable tool use.

\textbf{In-context learning vs.\ fine-tuning.} Few-shot prompts adapt behavior without training, at the cost of input tokens and brittle generalization. Fine-tuning bakes patterns into model weights, reducing prompt length and latency, but requires curated data, cost, and re-deployment.

\section{Reasoning and Planning}
\subsection{Chain of Thought (CoT)}
CoT prompting was introduced by Wei et al.~\cite{chainofthought} to improve reasoning tasks where the input–output mapping is non-trivial (e.g., math problem $\to$ numerical answer). The method adds intermediate reasoning steps in natural language, which bridge the gap between question and solution.
In benchmarks such as GSM8K and MultiArith, CoT significantly increased accuracy when compared to direct prompting. A refinement, \emph{Self-Consistency CoT} (CoT-SC), samples multiple reasoning chains and returns the most consistent answer, achieving further gains (e.g., PaLM 540B improved from 55\% $\to$ 74\% on GSM8K). However, CoT remains linear: each chain is generated without exploring alternatives.

\subsubsection{Few-Shot Chain-of-Thought \cite{chainofthought}}
Handcraft 8 examples with this fields:
Q: [Your question]
A: [Step-by-step reasoning]. The answer is [final answer].
Then use this for few-shot prompting. This is what made the paper achieve significantly increased accuracy.

\subsubsection{Zero-Shot Chain-of-Thought (“Let’s think step by step”) \cite{chainofzero}}
Q: [Your question here].  
Let’s think step by step.


The paper \cite{chainofzero} tested this prompts on the MultiArith dataset with text-davinci-002. Such prompts were such as "Let’s think about this logically" , " Before we dive into the answer", "Abrakadabra!"
The template "Let's think step by step" got the highest score 78,7 \% , while others such as "Abrakadabra!" got a 15,5 \% which surprisingly isn't far from raw prompt template which scored 15,5 \%.

\subsubsection{Conclusion}
Form the paper \cite{chainofzero}, the chain of thought proposed in \cite{chainofthought} was tested to evaluate the actual difference of using few-shot prompt with chain of thought.
Where the valuable mentions from the GSM8K benchmark, using PALM (540B) model \ref{tab:chainofzero}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|cc}
        \hline
        Method & GSM8K\\
        \hline
        Zero-Shot & 12,5 \\
        \hline
        Zero-Shot-CoT \cite{chainofzero} & 43,0 \\
        \hline
        Few-Shot-CoT \cite{chainofthought} & 56,9 \\
        \hline
    \end{tabular}
    \caption{Experiment curated results using PALM (540B) \ac{LLM} model \cite{chainofzero}}
    \label{tab:chainofzero}
\end{table}

According to the results see in the table above \ref{tab:chainofzero} in all the Few-Shot-CoT is still better. But the fact that Zero-ShoT-CoT is much easier to implement withouth having to come up with specific domain CoT examples. It is the best solution for the thesis for universal application.


\subsection{Tree-of-Thoughts (ToT)}
Yao et al.~\cite{treeofthought} extended CoT by introducing branching reasoning. Each intermediate step (a ``thought'') forms part of a search tree, where multiple candidate continuations can be generated, evaluated, and backtracked. This enables deliberate problem solving under uncertainty. On tasks such as the Game of 24, Mini Crosswords, and constrained story generation, ToT substantially outperformed both standard prompting and CoT (e.g., GPT-4 with ToT achieved 74\% accuracy on Game of 24, versus 19\% with CoT). The main trade-off is computational cost: ToT requires generating and evaluating many reasoning branches, making it slower and more resource-intensive.

\subsection{Reasoning \emph{and} Acting (ReAct)}
Yao et al.~\cite{react} proposed ReAct, a framework that interleaves reasoning traces with external actions (e.g., database queries, web search, or calculator calls) and observations. Unlike CoT, which is limited to internal reasoning, ReAct closes the loop between planning and evidence gathering, allowing the model to ground its reasoning in retrieved information. On knowledge-intensive QA tasks such as HotpotQA and FEVER, ReAct improved factual accuracy and significantly reduced hallucinations (e.g., from 14\% to 6\%). It also achieved strong performance on interactive environments like ALFWorld, where reasoning must be combined with sequential decision-making. The main trade-off is higher latency and token cost, since each step requires both reasoning tokens and tool interactions, but the gains in groundedness and reliability are substantial for retrieval-augmented applications.

\section{Tool Use and Orchestration}
\textbf{Self-taught tool use.} \emph{Toolformer} trains models to decide when and how to call APIs, improving arithmetic, lookup, and translation without heavy supervision.

\textbf{API-faithful calling.} \emph{Gorilla} fine-tunes LLMs with Retrieval-Aware Training (RAT) so the agent learns to consult evolving API docs at inference and reduces hallucinated calls.

\textbf{Model orchestration.} \emph{HuggingGPT} treats the LLM as a controller that plans subtasks and routes them to expert models; \emph{AutoGen} generalizes this to multi-agent conversations (LLM↔LLM↔Human↔Tools), useful for complex retrieval workflows (decomposition, retrieval, verification).

\section{Memory for Agents}
\textbf{Working vs.\ long-term memory.} Practical agents separate short context (scratchpad), episodic logs (events, prior answers), and semantic memory (facts, entities). \emph{MemGPT} formalizes tiered memory management beyond context windows.

\textbf{Reflective control.} \emph{Reflexion} adds a feedback loop: the agent critiques past trials and stores distilled lessons in memory, improving subsequent decisions without weight updates.

\section{Agentic RAG for Enterprise Retrieval}
\textbf{Problem.} Enterprise content lives in nested folders with metadata and ACLs. Pure dense retrieval over flat chunks ignores hierarchy, provenance, and permissions.

%\section{Evaluation}
%Agent performance should be measured beyond answer text:
%\begin{itemize}\itemsep2pt
    %\item \textbf{Task success on interactive environments:} \emph{AgentBench}, \emph{WebArena}, \emph{Mind2Web}, \emph{OSWorld}, and \emph{BrowserGym} cover planning, tool use, and real web/computer operations.
    %\item \textbf{Retrieval quality:} precision/recall@k, MRR/nDCG on document IDs (folder-aware); groundedness/faithfulness (LLM- or rule-based); citation accuracy to specific file paths and versions.
    %\item \textbf{Latency and cost:} hops, tool calls, tokens.
    %\item \textbf{Robustness and safety:} resistance to prompt/indirect-prompt injection and unsafe actions (see below).
%\end{itemize}
\section{Takeaways for This Thesis}
For enterprise semantic search, \ac{AAI} adds:
(i) \textbf{planning} to decompose queries and choose retrieval tools;
(ii) \textbf{hierarchical, metadata-aware retrieval} aligned with folders and ACLs;
(iii) \textbf{reflection} to re-query when evidence is weak;
(iv) \textbf{grounded generation} with verifiable citations. 
These capabilities directly target recall and precision in scattered document stores while preserving security and governance.

\section{Model Context Protocol (MCP)}

The \textbf{Model Context Protocol (MCP)} is an open standard introduced by Anthropic to connect large language models with external systems in a modular and reproducible way. It provides a universal interface---often described as a ``USB-C port for AI applications''---to integrate tools, data sources, and prompt templates without ad-hoc engineering \cite{mcp-intro}.

\subsection{Architecture} 
MCP follows a \textit{client--server model}. The \textit{host} (AI application) communicates through an MCP \textit{client} with one or more \textit{servers} that expose capabilities. Communication is structured in two layers:
(i) \textbf{Data layer:} uses JSON-RPC 2.0 to define primitives, discovery mechanisms, and lifecycle management;
(ii) \textbf{Transport layer:} handles message delivery (e.g., stdio for local, HTTP/SSE for remote) and authentication \cite{mcp-architecture}.


\subsection{Core Primitives} 
\begin{itemize}
  \item \textbf{Tools} --- typed, executable functions (e.g., search, API calls, database queries).
  \item \textbf{Resources} --- contextual data the model can access (files, records, metadata).
  \item \textbf{Prompts} --- versioned templates or workflows to standardize interactions.
  \item \textbf{Memory} --- structured persistence of state, supporting short-term caches and long-term profiles.
\end{itemize}

MCP contributes to the shift toward \textit{agentic LLMs}, where models not only generate text but also plan, act, and learn via external capabilities. Its main benefits include standardization, by providing a unified interface across backends; reproducibility, through versioned prompts and schemas; extensibility, since new tools and resources can be added without redesign; and safety, as schema validation and scoped access reduce errors \cite{mcp-spec}.
