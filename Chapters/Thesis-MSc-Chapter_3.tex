% #############################################################################
% This is Chapter 3
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{State of the Art}
\label{chap:stateofart}
\cleardoublepage

Building on the foundational retrieval methods introduced in Chapter~\ref{chap:back}, this chapter surveys \gls{AAI}—language-model driven agents that plan, use tools, and manage memory to improve retrieval and answer quality. We focus on methods that (i) reason and decompose tasks, (ii) call external tools and data sources, and (iii) maintain working memory and long-term context. These agentic capabilities address the challenges enterprises face with semantic search (Chapter~\ref{chap:intro}) by adding planning and reasoning to retrieval-augmented generation (\gls{RAG}) pipelines that must respect organizational structure (folders, metadata) and access control.

\section{From Prompting to Agentic Behavior}

At their core, \glspl{LLM} are next-token predictors. Yet, with careful prompt design and light scaffolding, we can elicit behaviors that look like planning, tool use, and memory—turning a general model into an agent that can reason, act, and operate under constraints. In practice, this progression typically moves through three layers: (i) instruction and context shaping, (ii) structured reasoning, and (iii) controlled action via external tools and state. The rest of this chapter deepens each layer through Chain-of-Thought, Tree-of-Thoughts, and ReAct, and then discusses orchestration and protocol standards such as MCP.

\textbf{Prompt engineering.} Clear, task-oriented instructions shape how models respond: roles (``You are a compliance analyst''), goals (``decide yes/no and explain''), constraints (``answer in JSON with fields X, Y''), style, and evaluation criteria. Context windows can be filled with distilled background (summaries, definitions, schemas) and the dialog history can be periodically summarized and re-provided to keep the model on track. Output schemas (e.g., few fields of JSON or a fixed template) further stabilize format and make downstream automation reliable. Prompting thus controls tone, structure, and short-term domain grounding. However, it does not by itself ensure factuality, robust adherence to tools or APIs, or reproducibility under distribution shifts; prompts remain brittle when the task departs from seen patterns.

\textbf{In-context learning vs.\ fine-tuning.} Few-shot exemplars teach models how to behave by patterning on demonstrations placed directly in the prompt. This enables rapid adaptation without training but increases token usage and can generalize brittlely beyond the demonstrated cases. Fine-tuning, in contrast, bakes those patterns into the model weights, reducing prompt length and latency while improving stability on the tuned behaviors. The trade-offs are cost (data curation and training), operational complexity (versioning and re-deployment), and potential drift if the underlying APIs or schemas change. In this thesis we prefer minimal, reusable prompt patterns combined with retrieval of fresh context and explicit tool schemas; targeted fine-tuning is reserved for behaviors that must be consistent at scale.

\section{Reasoning and Planning}
\subsection{Chain of Thought (CoT)}
CoT prompting, introduced by Wei et al.~\cite{chainofthought}, elicits an explicit sequence of intermediate \emph{rationales} $r=(r_1,\ldots,r_T)$ before producing the final answer $y$. Instead of directly mapping $x\to y$, the prompt conditions the model to first generate a plausible reasoning chain and then a concise answer. From a probabilistic view, the ideal predictor marginalizes over latent rationales,
\begin{equation}
 y^* = \arg\max_y \sum_r p(y,r\mid x),
\end{equation}
and CoT steers decoding to sample a high-probability $r$ followed by $y$. In practice, few-shot demonstrations of the pattern ``step-by-step reasoning $\Rightarrow$ final answer'' shape the token distribution so that the model decomposes the task (arithmetic sub-results, constraint checks, commonsense implications), reducing search depth and concentrating probability mass on correct completions.

	\textbf{Why it helps.} CoT is effective on compositional problems (multi-step arithmetic, symbolic manipulation, multi-hop QA) because: (i) it decomposes the problem into locally verifiable steps; (ii) it exposes intermediate constraints to the model, enabling self-correction within the chain; and (iii) it regularizes decoding to match the stepwise structure seen in demonstrations. Empirically, CoT improves accuracy on GSM8K, MultiArith, and related benchmarks.

	\textbf{Limitations.} Vanilla CoT is \emph{linear}: a single chain is generated left-to-right; early errors can propagate; and a single sample may miss alternative valid paths. This motivates ensembling (Self-Consistency) and structured search (Tree-of-Thoughts), discussed below.

\subsubsection{Few-Shot Chain-of-Thought \cite{chainofthought}}
\label{subsec:Few-ShotCoT}
Handcraft 8 examples with these fields:

\begin{lstlisting}[language={},basicstyle=\ttfamily\footnotesize,caption={Few-Shot Chain-of-Thought Prompt Template}]
Q: [Your question]\\
A: [Step-by-step reasoning]. The answer is [final answer].
\end{lstlisting}

Then use this for few-shot prompting. This is what made the paper achieve significantly increased accuracy.

\subsubsection{Self-Consistency (CoT-SC)}
\label{subsec:cot-sc}
Self-Consistency addresses the ``single-chain'' limitation by sampling multiple independent CoT chains and selecting the most consistent final answer. Algorithmically:
\begin{enumerate}
    \item Use a CoT-style prompt (few-shot or zero-shot) that ends with a clearly parseable final answer (e.g., ``The answer is [\dots]'').
    \item Sample $k$ chains with stochastic decoding (e.g., temperature or nucleus sampling).
    \item Parse the final answers $\{y^{(j)}\}_{j=1}^{k}$ from the chains.
    \item Return the majority answer; break ties by likelihood or a lightweight verifier.
\end{enumerate}
This procedure approximates the marginalization $\arg\max_y \sum_r p(y,r\mid x)$ via Monte Carlo sampling of $r$. It yields substantial gains (e.g., PaLM 540B on GSM8K from 55\% to 74\%), with token/latency cost roughly linear in $k$.

\subsection{Tree-of-Thoughts (ToT)}
Yao et al.~\cite{treeofthought} extended CoT by introducing branching reasoning. Each intermediate step (a ``thought'') forms part of a search tree, where multiple candidate continuations can be generated, evaluated, and backtracked. This enables deliberate problem solving under uncertainty. On tasks such as the Game of 24, Mini Crosswords, and constrained story generation, ToT substantially outperformed both standard prompting and CoT (e.g., \gls{GPT}-4 with ToT achieved 74\% accuracy on Game of 24, versus 19\% with CoT). The main trade-off is computational cost: ToT requires generating and evaluating many reasoning branches, making it slower and more resource-intensive.

\textit{Relation to CoT-SC.} Both ToT and Self-Consistency explore \emph{multiple} reasoning paths. CoT-SC does so via independent samples of linear chains with a majority vote over final answers, approximating $\arg\max_y \sum_r p(y,r\mid x)$. ToT conducts a \emph{guided} search over a tree of partial solutions using generation plus heuristic evaluation and backtracking. This can focus computation on promising branches but typically incurs higher coordination and token cost.

The tree in ToT is made up of nodes, which represent states ($s=[x, z_{1,\ldots,i}]$), that are partial solutions to the problem, with input the thoughts thus far. To generate states, it takes four steps. \textbf{Thought Decomposition}: a thought can be a a line of equation (Game of 24), a couple of words (crosswords),  a full paragraph of a writing plan (Creative Writing), or a class in a structured organizational folder (corporate structural search). Which is composed to a certain length so that a \glspl{LLM} can process it. \textbf{Thought Generator}, given a state ($s=[x, z_{1,\ldots,i}]$), the model generates the next thoughts $G(p_\theta,s,k)$. The paper ToT mentions two strategies:
\begin{itemize}
    \item Sample thoughts from a CoT prompt~\ref{subsec:Few-ShotCoT}. This strategy works better for diverse and creative writing tasks,  where the thoughts take the form of full paragraphs.
    \begin{equation}
        z^{j}  \sim p_\theta^{CoT}(z_{i+1}\mid s) = p_\theta^{CoT}(z_{i+1}\mid x, z_{1,\ldots,i})\; (j = 1\,\ldots\,k)
    \end{equation}
    \item  Propose thoughts sequentially using a ``propose prompt''. This works better when the thought space is constrained, when the thoughts are just a word or a line.
    \begin{equation}
    [z_{i+1}^{(1)}, \dots, z_{i+1}^{(k)}] \sim p_\theta^{\text{propose}}(z_{i+1}^{(1:k)} \mid x, z_1, \dots, z_i)
    \end{equation}
\end{itemize}

The next component is the \textbf{state evaluator} $V(p_\theta,S)$, which heuristically assesses whether a state has reached the goal. Which then determines if it is valuable to keep exploring this chain of thought. ToT \cite{chainofthought} uses a \gls{LLM} to evaluate.
Then to further the tree ToT uses simple search algorithms in the tree structure such as Breadth-first search (BFS) and Depth-first search (DFS).

The contribution of this paper to the thesis lies in demonstrating how an \gls{LLM} can be orchestrated to navigate the organizational structure in a semantically aware manner.

\subsection{Zero-Shot Chain-of-Thought (``Let's think step by step'')~\cite{chainofzero}}
\begin{lstlisting}[language={},basicstyle=\ttfamily\footnotesize,caption={Zero-Shot Chain-of-Thought Prompt Template}]
Q: [Your question here].\\
Let's think step by step.
\end{lstlisting}

The paper~\cite{chainofzero} tested several zero-shot reasoning cues on the MultiArith dataset with \texttt{text-davinci-002} (e.g., ``Let's think about this logically'', ``Before we dive into the answer'', ``Abrakadabra!''). The prompt ``Let's think step by step'' achieved the highest score (78,7\%), while unrelated cues like ``Abrakadabra!'' performed near the raw baseline (15,5\%). The takeaway is that explicitly inviting step-by-step reasoning can elicit a chain of thought even without few-shot exemplars.

\subsection{Conclusion}
Zero-shot CoT offers a low-effort way to elicit reasoning; Few-shot CoT provides stronger improvements when domain-specific exemplars are available; CoT-SC ensembles multiple chains to better approximate marginalization over rationales; and ToT adds branching search for deliberate problem solving at higher computational cost. In this thesis we prefer Zero-Shot-CoT for broad applicability, and strategies that resemble branching are also explored for improving queries to retrieve context.

Representative results on GSM8K with PaLM (540B) are shown in Table~\ref{tab:chainofzero}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        Method & GSM8K\\
        \hline
    Zero-Shot & 17,9~\cite{chainofthought} \\
        Zero-Shot-CoT~\cite{chainofzero} & 43,0 \\
        Few-Shot-CoT~\cite{chainofthought} & 56,9 \\
        CoT-SC (Self-Consistency)~\cite{chainofthought} & 74,0 \\
        \hline
    \end{tabular}
    \caption{Selected experimental results using PaLM (540B) on GSM8K~\cite{chainofthought}.}
    \label{tab:chainofzero}
\end{table}

\subsubsection{Note on comparability.} In the ToT study (Yao et al.~\cite{treeofthought}), GSM8K was evaluated with \gls{GPT}-4 in a zero-shot setup on a 100-question subset. \textbf{IO} (Input\textendash Output) denotes the standard direct prompting baseline without step-by-step rationales (i.e., no CoT). Because the model and evaluation protocol differ from the PaLM (540B) setup above, we report those numbers separately for clarity.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        Method & GSM8K\\
        \hline
        IO (direct prompting)~\cite{treeofthought} & 51,0 \\
        CoT~\cite{treeofthought} & 86,0 \\
        ToT (zero-shot, GPT-4)~\cite{treeofthought} & 90,0 \\
        \hline
    \end{tabular}
    \caption{GSM8K results reported by Yao et al.~\cite{treeofthought} using GPT-4 in a zero-shot ToT setup (100-question subset). Results are shown separately because the model and evaluation protocol differ from the PaLM (540B) setup above.}
    \label{tab:tot-gpt4-gsm8k}
\end{table}

\subsection{Reasoning \emph{and} Acting (ReAct)}
Yao et al.~\cite{react} propose ReAct, a prompting framework that \emph{interleaves} short natural-language \textit{thoughts} with concrete \textit{actions} and observed evidence. At each step the model reasons about what to do next, calls a tool (e.g., Wikipedia search, database query, calculator), receives an \textit{observation}, and updates its working state (the history of thoughts, actions, and observations). In contrast to purely internal Chain-of-Thought (CoT), ReAct closes the loop between planning and evidence gathering, which reduces hallucinations by grounding intermediate claims in retrieved information and enables recovery from early mistakes via subsequent actions.

Concretely, ReAct was evaluated on knowledge-intensive QA and fact verification by coupling the LLM with a simple Wikipedia API on \textbf{\textit{HotpotQA}} and \textbf{\textit{FEVER}}. It also tested sequential decision making in \textbf{ALFWorld} and \textbf{WebShop}. Compared to CoT-style baselines, ReAct improves verification accuracy and produces human-interpretable trajectories (reasoning + actions + observations). The main trade-off lies in increased latency and token cost, since each step emits reasoning tokens and performs tool I/O. In retrieval-augmented settings, however, the gains in groundedness and reliability are substantial.



\subsubsection{Example}
ReAct interleaves reasoning traces with external actions and observations. The framework follows this pattern:

\begin{lstlisting}[language={},basicstyle=\ttfamily\footnotesize,caption={ReAct Framework Pattern}]
Thought: [Reasoning about what to do next]
Action: [Specific action to take, e.g., search, calculate, query]
Observation: [Result of the action]
Thought: [Next reasoning step based on observation]
...
\end{lstlisting}

For example, to answer "What is the population of Paris and how does it compare to London?":

\begin{lstlisting}[language={},breaklines=true,basicstyle=\ttfamily\footnotesize,caption={ReAct Example: Population Comparison Query}]
Q: What is the population of Paris and how does it compare to London?
Thought: I need to find the current population of both cities to make a comparison. I'll start by 
searching for Paris's population.
Action: Search[Paris population 2023]
Observation: Paris has approximately 2.16 million inhabitants in the city proper (2023 estimate).
Thought: Now I need London's population for comparison.
Action: Search[London population 2023]
Observation: London has approximately 8.98 million inhabitants (2023 estimate).
Thought: Paris has about 2.16 million people while London has 8.98 million, so London is roughly 4 
times larger.
Action: Calculate[8.98 / 2.16]
Observation: 8.98 $\div$ 2.16 $\approx$ 4.16
Thought: The calculation confirms London is about 4.16 times larger than Paris.
Final Answer: Paris has approximately 2.16 million inhabitants, while London has 8.98 million, 
making London about 4.16 times larger than Paris.
\end{lstlisting}

This interleaving of reasoning (Thought) with external actions (Action) and observations allows ReAct to ground its responses in retrieved information, rather than relying solely on parametric knowledge stored in model weights, thereby reducing hallucinations while improving factual accuracy.

\paragraph{Formalization.} Given a question or task $x$, define the agent state at step $t$ as $s_t = (x, h_t)$, where $h_t$ is the history of prior thoughts, actions, and observations. The agent alternates between emitting a \textit{Thought} $\tau_t$ and an \textit{Action} $a_t$:
\begin{itemize}
    \item Thought: $\tau_t = f_\theta(\text{``Think''} \mid s_t)$ produces a short rationale that decides the next operation.
    \item Action: $a_t \in \mathcal{A}$ is executed against an external environment $\mathcal{E}$ (e.g., search, retrieve, calculate), yielding observation $o_t = \mathcal{E}(a_t)$.
\end{itemize}
The history is updated $h_{t+1} = h_t \cup \{\tau_t, a_t, o_t\}$ and the loop repeats until a termination condition emits a \textit{Final Answer}. A simple ReAct policy thus alternates $\text{Thought} \rightarrow \text{Action} \rightarrow \text{Observation}$, using observations to correct or refine subsequent thoughts.

\paragraph{Algorithmic reference.} A formal pseudocode of this loop is provided in Appendix B (Section~\ref{alg:react-loop}); we refer readers there for the step-by-step algorithm.

\paragraph{Reported results.} Using PaLM-540B with a Wikipedia API, the ReAct paper reports (first value HotpotQA Exact Match (EM), second value FEVER accuracy): Standard (28.7, 57.1), CoT~\cite{chainofthought} (29.4, 56.3), CoT-SC~\cite{chainofthought} (33.4, 60.4), Act-only (25.7, 58.9), ReAct (\textbf{27.4}, \textbf{60.9}), CoT-SC$\,\Rightarrow$ReAct (\textbf{34.2}, \textbf{64.6}), ReAct$\,\Rightarrow$CoT-SC (35.1, 62.0). Error analysis shows hallucinated reasoning traces or facts are substantially lower for ReAct than CoT (\textbf{6\%} vs. \textbf{14\%}) in the successful predictions breakdown. On interactive environments, ReAct achieves strong gains (e.g., ALFWorld average success rate up to \textbf{71\%}, beating best Act at 45\% and BUTLER at 37\%; absolute improvements of \textbf{+34\%} on ALFWorld and \textbf{+10\%} on WebShop over imitation/reinforcement learning baselines)~\cite{react}.


\section{Tool Use and Orchestration}

While ReAct already integrates tool use within its action step, this section focuses on how such capabilities can be further optimized for accuracy.  
Tool-use frameworks aim to improve how language models identify, select, and execute external tools, thereby reducing hallucinations and minimizing execution errors.  

For instance, consider a WebCommerce assistant that interfaces with an e-commerce database, enabling customers to place new orders, track deliveries, or request refunds directly through conversation.
Each of these actions relies on precise and validated tool calls to backend APIs—such as \texttt{createOrder()}, \texttt{getOrderStatus()}, or \texttt{updatePayment()}—where incorrect parameters or schema mismatches can lead to failed transactions or inconsistent records, emphasizing the importance of accurate and validated orchestration. 

In such a setting, precision in tool call is crucial: misunderstood or malformed parameters can cause transactions failures or data inconsistencies—highlighting the necessity of reliable tool intefration  mechanisms.

\textbf{Toolformer}~\cite{schick2023toolformer} introduced the concept of \emph{self-taught tool use}.  
It fine-tunes a language model (based on \gls{GPT}-J)through a self-annotation loop: during training, the model is prompted to insert possible API calls into text, these calls are executed, the returned outputs are evaluated based on whether they reduce the model's prediction loss over masked tokens.
If an API’s result helps the model predict the next token more accurately, that example is kept as supervision.  
This process allows the model to learn when a tool should be invoked and how to integrate its output into natural language responses.  

Although Toolformer improved factual accuracy by roughly 60\% on arithmetic and translation tasks.

\textbf{Gorilla}~\cite{patil2023gorilla} expanded on this idea with \emph{Retrieval-Aware Training (RAT)}.  
Instead of memorizing tool usage, Gorilla embeds both API documentation (function signatures, parameter types, examples) and natural-language queries into a joint vector space using \emph{contrastive learning}. This method pushes matching pairs (query $\leftrightarrow$ correct API) closer together while pushing unrelated pairs apart.  
During inference time, the model retrieves the most semantically similar API specification and conditions its text generation on that retrieved context, producing syntactically valid function calls through templated generation constrained by the retrieved schema.  
On the \textit{APIBench} benchmark, Gorilla achieved \textbf{90–92\% API-call correctness} and reduced hallucinated calls by over \textbf{40\%} compared to Toolformer.  
However, like Toolformer,  it still requires re-training when entirely new APIs are introduced, which limits adaptability.

\textbf{HuggingGPT}~\cite{shen2023hugginggpt} reframed the orchestration as \emph{model routing}.  
It treats the LLM as a planner that decomposes complex user requests into subtasks and, for each subtask, queries a catalog of expert models hosted on the Hugging Face hub.  
Although this demonstrated cross-modal reasoning across text, vision, and audio tasks, the approach introduced limited methodological novelty beyond Gorilla’s retrieval step.  
The evaluation was conducted via human assessment on 130 user requests, where the best-performing model (\gls{GPT}-3.5) achieved a model-selection passing rate of \textbf{93.89\%}, a rationality score of \textbf{84.29\%}, and a final-response success rate of \textbf{63\%}.



\textbf{AutoGen}~\cite{wu2023autogen} generalized the orchestration into a \emph{multi-agent dialogue framework}.  
In this paradigm, multiple LLMs and tools interact conversationally, exchanging natural-language messages that represent plans, results, or verifications.  
Each agent can reason (\emph{Chain-of-Thought}), act (\emph{ReAct}), or delegate subtasks to others, enabling cooperative problem-solving.  
Benchmarking on reasoning environments such as \textit{ALFWorld} and \textit{HotpotQA} reported up to \textbf{17\% higher task success rates} than single-agent baselines, confirming that collaboration and verification loops improve reliability.  
Nonetheless, AutoGen is computationally expensive and often unstable: extended conversation loops between agents can lead to significant inefficiency or even infinite exchanges without task resolution.  
Moreover, it lacks a standardized interface for connecting to external systems, making integration with enterprise infrastructures or databases cumbersome.

\textbf{Model Context Protocol (MCP)} introduces a standardized framework for connecting LLMs with external systems through explicit typed interfaces.  
Unlike previous approaches that relied on learned behavior or conversational coordination, MCP formalizes tool interaction by exposing capabilities through structured \texttt{tools}, \texttt{resources}, and \texttt{prompts}.  
Each is defined by a JSON schema that specifies valid inputs and outputs, allowing clients to discover available operations and invoke them using JSON-RPC calls.  
This replaces informal prompt-based conventions with deterministic, schema-validated communication, ensuring consistency and interoperability across agents and environments.  
The following section describes MCP’s architecture in detail and how it was applied in this thesis to integrate Weaviate with LLM agents.


\section{Model Context Protocol (MCP)}

The \textbf{Model Context Protocol (MCP)} is an open standard introduced by Anthropic to connect large language models with external systems in a modular and reproducible way. It provides a universal interface---often described as a ``USB-C port for AI applications''---to integrate tools, data sources, and prompt templates without ad-hoc engineering \cite{mcp-intro}.

\subsection{Architecture} 
MCP follows a \textit{client--server model}. Where the \textit{host} (AI application) communicates through an MCP \textit{client} with one or more \textit{servers} that expose capabilities. Communication is structured in two layers:
(i) \textbf{Data layer:} uses JSON-RPC 2.0 to define primitives, discovery mechanisms, and lifecycle management;
(ii) \textbf{Transport layer:} handles message delivery (e.g., stdio for local, HTTP/SSE for remote) and authentication \cite{mcp-architecture}.


\subsection{Core Primitives} 
\begin{itemize}
  \item \textbf{Tools} --- typed, executable functions (e.g., search, API calls, database queries).
  \item \textbf{Resources} --- contextual data the model can access (files, records, metadata).
  \item \textbf{Prompts} --- versioned templates or workflows to standardize interactions.
  \item \textbf{Memory} --- structured persistence of state, supporting short-term caches and long-term profiles.
\end{itemize}

MCP contributes to the shift toward \textit{agentic LLMs}, where models not only generate text but also plan, act, and learn through external capabilities.  
It has rapidly become a key standard for connecting AI applications to external systems, with implementations as an MCP client in platforms such as Docker, Claude Desktop, and VS Code, and supported by multiple servers including MongoDB, Google Workspace, and LangChain.  
Additional open-source implementations include the \texttt{docker/mcp-toolkit}, \texttt{anthropic/mcp}, and \texttt{mongodb-js/mcp-server}.  
One of these servers—the \textbf{Weaviate MCP server}—was developed in this thesis to expose Weaviate’s schema and query interface as callable MCP \texttt{tools}.  

MCP’s main advantages are its \textbf{standardization} —providing a unified protocol across heterogeneous backends; \textbf{reproducibility}— through versioned prompts and schema-defined tools; \textbf{extensibility}, allowing new capabilities to be added without architectural changes. \cite{mcp-spec}.  
In this thesis, MCP functions as the interoperability layer between Weaviate and LLM agents, enabling schema-aware retrieval and controlled reasoning over enterprise information in a reproducible and standardized manner.

\section{Related Work}

\section{Summary}
This chapter traced the progression from prompting to agentic behavior and motivated ReAct—interleaving Thought, Action, and Observation—as the pattern we adopt in this thesis (formal loop in Appendix~B, Section~\ref{alg:react-loop}). In practice, agents benefit from many tools. Contemporary assistants such as Copilot and Claude already provide file-backed tools (e.g., project-local \texttt{TODO.md} and lightweight Markdown scratchpads) to externalize short plans and track progress across turns. We do not reimplement these tools; instead, we ensure our solution is compatible so they can be used alongside our retrieval components, improving the reliability of semantic search tasks.

To enable this interoperability, we expose Weaviate via the Model Context Protocol (MCP). We developed a Weaviate MCP server that provides schema-aware search and fetch as typed tools. In our implementation, this MCP interface is used with contemporary agent frameworks—initially a LangChain agent and later an OpenAI tool-calling agent, following its recent release—for benchmarking and for structured, schema-informed cross-references during semantic search. Because MCP is a common, typed interface, any MCP-compatible agent—including those with built-in tools like TODO lists and scratchpads—can plug our server in immediately and extend it with additional tools over time, leveraging the latest Agentic AI tooling without bespoke adapters.