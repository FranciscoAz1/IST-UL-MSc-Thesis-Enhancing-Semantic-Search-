% #############################################################################
% Abstract Text
% !TEX root = ../main.tex
% #############################################################################
% reset acronyms
\glsresetall
% use \noindent in firts paragraph
\noindent  
This thesis designs and implements a retrieval‑augmented, agentic semantic search system for corporate document management, developed with Edoclink. Corporate repositories carry rich metadata and workflows that go beyond keyword search.
We explore an example newsroom scenario in which journalists query a document repository to fact‑check in‑progress articles; the system retrieves relevant documents, follows cross‑references to perform programmable multi‑hop retrieval over the repository’s structure, and synthesizes cited answers to support efficient, accurate verification.

We leverage advances in \glsxtrfull{NLP}, centering on \glsxtrfull{RAG} and its extension into \glsxtrfull{AAI}. The system pipeline (i) ingests and normalizes heterogeneous documents, (ii) indexes content in a \glsxtrfull{VD} with metadata‑aware schemas, and (iii) uses \glsxtrfullpl{LLM} orchestrated via \glsxtrfull{PE} to ground answers in retrieved evidence. We study retrieval in isolation—structure‑aware/hierarchical chunking, metadata filtering, and hybrid sparse and dense search—and then implement Agentic RAG, where specialized agents perform query analysis, collection selection, iterative retrieval, tool use, programmable multi‑hop graph traversal, and cited synthesis across distributed sources.
We integrate a scalable, ACL‑aware \gls{VD} with an \glsxtrfull{MCP} server that exposes origin queries and follow‑reference tools, enabling structured graph traversal within \gls{AAI} systems, minimizing context‑window usage and improving retrieval efficiency. We validate the approach on representative enterprise datasets aligned with Edoclink’s use cases and report design and evaluation results—answer quality, traceability, latency, and cost—providing a deployable blueprint for enterprise settings. To quantify correctness, we calibrate and apply thresholds across lexical, structural, and semantic metrics using Youden’s J statistic to classify answers as correct or incorrect.






