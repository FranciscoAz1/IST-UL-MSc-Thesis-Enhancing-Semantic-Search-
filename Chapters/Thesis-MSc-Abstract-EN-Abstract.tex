% #############################################################################
% Abstract Text
% !TEX root = ../main.tex
% #############################################################################
% reset acronyms
\glsresetall
% use \noindent in firts paragraph
\noindent  
This thesis proposes and implements a retrieval‑augmented, agentic semantic search system for corporate document management, developed in collaboration with Edoclink. Corporate repositories contain rich metadata and workflows structures that extend beyond traditional keyword search.
The system is evaluated through a newsroom scenario  where journalists query the document repository to fact‑check ongoing articles; the system retrieves relevant documents, navigates cross‑references to perform programmable multi‑hop retrieval across the repository’s structure, and synthesizes cited answers to support efficient and accurate verification.

We leverage advances in \glsxtrfull{NLP}, focusing on \glsxtrfull{RAG} and its extension toward \glsxtrfull{AAI}. The system pipeline (i) ingests and normalizes heterogeneous documents, (ii) indexes content in a \glsxtrfull{VD} using metadata‑aware schemas, and (iii) employs \glsxtrfullpl{LLM} orchestrated via \glsxtrfull{PE}, to ground answers in retrieved evidence. We study retrieval in isolation—structure‑aware and hierarchical chunking, metadata filtering, and hybrid sparse and dense search—and then develop Agentic RAG, in which specialized agents perform query analysis, collection selection, iterative retrieval, tool use, programmable multi‑hop graph traversal, and cited synthesis across distributed sources.
We propose and integrate a scalable, ACL‑aware \gls{VD} with an \glsxtrfull{MCP} server that exposes origin queries and follow‑reference capabilities, enabling structured graph traversal for \gls{AAI} systems, thereby minimizing context‑window consuption and improving retrieval efficiency. We validate the approach on representative enterprise datasets reflecting Edoclink’s use cases and report design and evaluation results—answer quality, traceability, latency, and cost—providing a deployable blueprint for enterprise settings. To quantify correctness, we calibrate and apply thresholds across lexical, structural, and semantic metrics using Youden’s J statistic to classify answers as correct or incorrect.






