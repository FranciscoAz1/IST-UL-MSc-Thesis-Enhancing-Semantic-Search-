% #############################################################################
% This is Chapter 1
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Introduction}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:intro}
\section{Contextual Background}
The topic of this thesis arises from the context of Edoclink, a cloud-based business management platform developed by Link Consulting.\footnote{\url{https://linkconsulting.com/what-we-do/products/edoclink-v8/}} Edoclink is designed to support organizations in managing their information in an effective, autonomous, and scalable manner, through the integration of advanced artificial intelligence techniques. The platform offers a broad range of functionalities for document management, workflow automation, and general information governance, making it increasingly feature-rich and adaptable to diverse operational needs.

The recent advancements in natural language processing (\gls{NLP}), particularly with the emergence of large language models (\glsxtrfullpl{LLM}), have introduced new paradigms in document processing. These models are capable of producing contextually rich and semantically meaningful outputs, even from unstructured data sources. As such, they enable a new class of intelligent information systems that go beyond traditional keyword-based approaches, offering enhanced information extraction, retrieval, and summarization capabilities.

Due to confidentiality constraints, the specific clients using Edoclink cannot be disclosed. Nevertheless, the work presented in this thesis is developed in a generalized context, with the aim of addressing typical challenges faced in enterprise information management. In particular, emphasis is placed on ensuring that the proposed solution is scalable in highly dynamic environments and maintains high interoperability with existing systems and standards.

% #############################################################################
\section{Motivation}

In recent years, the development of \glsxtrfull{LLM}, such as ChatGPT, LLama, Gemini, etc., has marked a significant milestone in the evolution of \gls{NLP}. These models stand out for their ability to understand and generate text in natural language with high precision and context scale. These advancements were made possible by the introduction of the Transformer architecture \cite{vaswani2017attention}.

The Transformer revolutionized the field by introducing advanced self-attention mechanisms and scalability, enabled by its highly parallelizable structure. Unlike previous architectures such as \glsxtrfull{RNN} and \glsxtrfull{LSTM}, which rely on sequential processing, the Transformer uses multi-head attention and parallel operations to capture long-range dependencies in large datasets more efficiently. This innovation is the technological foundation of modern \gls{LLM} like ChatGPT, LLama, Gemini, etc., allowing them to handle much more complex, diverse, and large-scale contexts than sequential types.

\gls{LLM} excel not only in their ability to understand and generate text with high accuracy but also in their ability to process large volumes of information, with complex \gls{RAG} systems. That use embeddings to process text in to vectors better indexed with \gls{RAG}, making them indispensable tools for organizations looking to transform large amount of information into useful knowledge. Their implementation enables more efficient and contextualized interactions with organizational data, offering precise responses to complex queries and optimizing decision-making processes based on information.

The combined use of \gls{RAG} and \gls{LLM} represents a promising solution to overcome these challenges. Models like ChatGPT and LLama which can be embedded with \gls{RAG} have demonstrated high performance in the contextualized retrieval of information and responding to complex queries. However, implementing these models in a corporate context requires a customized approach that respects information privacy and confidentiality, integration with existing organized information, and is scalable to meet the specific needs of each client. Additionally, it is necessary to ensure that the models can operate efficiently in local environments, ensuring that security and compliance with regulations, such as \glsxtrfull{GDPR}, are fully met.

\section{Objectives}
This thesis aims to design and implement a lightweight yet functional architecture for improving search and organization of information in company environments. The proposed system integrates a set of components and services with the following main objectives:
\begin{itemize}
\item \textbf{Decentralized Semantic Search:} Develop a system that enables decentralized semantic search across multiple databases of information, ensuring accessibility and scalability.
\item \textbf{Retrieval-Augmented Generation (RAG) Pipeline:} Implement a complete RAG pipeline capable of processing and indexing documents. This pipeline will extract and store document metadata, and semantic embeddings enabling semantic retrieval efficiently.
\item \textbf{Metadata Extraction and Organization Suggestions:} Automatically extract metadata from documents to improve search accuracy and suggest organizational structures for better document management.
\item \textbf{Internet-Aware Search Capabilities:} Explore the integration of external sources and databases over the internet to enable the system to retrieve information beyond the local knowledge base, when appropriate.
\item \textbf{Agentic Search System:} Implement a chatbot interface for document search, that is capable of easily connecting to external sources of information. Enabling conversational search of documents.
\end{itemize}
Together, these objectives form the basis of a robust, scalable, and cost-efficient information retrieval system tailored for use in dynamic and distributed company environments.
% #############################################################################
\section{Organization of the Document}

\Cref{chap:back} presents the theoretical background required to understand the developed work, including fundamental concepts of information retrieval, vector representations, and large language models. 

\Cref{chap:State_of_Art} reviews the state of the art in agentic artificial intelligence, highlighting reasoning frameworks and tool-use paradigms that inspired the proposed architecture. 


In \Cref{chapter:System_Arquitecture}, the overall system architecture is detailed, describing how document processing, schema design, and retrieval components integrate into a unified solution.

\Cref{chapter:Results} reports the experimental setup, performance evaluation, and discussion of the obtained results, comparing different retrieval strategies and assessing their effectiveness. 

Finally, \Cref{chap:conclusion} concludes the dissertation by summarizing the main contributions, identifying system limitations, and outlining future research directions.
