% #############################################################################
% This is Chapter 1
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Introduction}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:intro}
\section{Contextual Background}
The topic of this thesis arises from the context of Edoclink, a cloud-based business management platform developed by Link Consulting.\footnote{\url{https://linkconsulting.com/what-we-do/products/edoclink-v8/}} Edoclink is designed to support organizations in managing their information in an effective, autonomous, and scalable manner, through the integration of advanced artificial intelligence techniques. The platform offers a broad range of functionalities for document management, workflow automation, and general information governance, making it increasingly feature-rich and adaptable to diverse operational needs.

The recent advancements in natural language processing (NLP), particularly with the emergence of large language models (\glsxtrfullpl{LLM}), have introduced new paradigms in document processing. These models are capable of producing contextually rich and semantically meaningful outputs, even from unstructured data sources. As such, they enable a new class of intelligent information systems that go beyond traditional keyword-based approaches, offering enhanced information extraction, retrieval, and summarization capabilities.

Due to confidentiality constraints, the specific clients using Edoclink cannot be disclosed. Nevertheless, the work presented in this thesis is developed in a generalized context, with the aim of addressing typical challenges faced in enterprise information management. In particular, emphasis is placed on ensuring that the proposed solution is scalable in highly dynamic environments and maintains high interoperability with existing systems and standards.

% #############################################################################
\section{Motivation}

In recent years, the development of \glsxtrfull{LLM}, such as ChatGPT, LLama, Gemini, etc., has marked a significant milestone in the evolution of \glsxtrfull{NLP}. These models stand out for their ability to understand and generate text in natural language with high precision and context scale. These advancements were made possible by the introduction of the Transformer architecture \cite{vaswani2017attention}.

The Transformer revolutionized the field by introducing advanced self-attention mechanisms and scalability, enabled by its highly parallelizable structure. Unlike previous architectures such as \glsxtrfull{RNN} and \glsxtrfull{LSTM}, which rely on sequential processing, the Transformer uses multi-head attention and parallel operations to capture long-range dependencies in large datasets more efficiently. This innovation is the technological foundation of modern \glsxtrfull{LLM} like ChatGPT, LLama, Gemini, etc., allowing them to handle much more complex, diverse, and large-scale contexts than sequential types.

\glsxtrfull{LLM} excel not only in their ability to understand and generate text with high accuracy but also in their ability to process large volumes of information, with complex \glsxtrfull{RAG} systems. That use embeddings to process text in to vectors better indexed with \glsxtrfull{RAG}, making them indispensable tools for organizations looking to transform large amount of information into useful knowledge. Their implementation enables more efficient and contextualized interactions with organizational data, offering precise responses to complex queries and optimizing decision-making processes based on information.

The combined use of \glsxtrfull{RAG} and \glsxtrfull{LLM} represents a promising solution to overcome these challenges. Models like ChatGPT and LLama which can be embedded with \glsxtrfull{RAG} have demonstrated high performance in the contextualized retrieval of information and responding to complex queries. However, implementing these models in a corporate context requires a customized approach that respects information privacy and confidentiality, integration with existing organized information, and is scalable to meet the specific needs of each client. Additionally, it is necessary to ensure that the models can operate efficiently in local environments, ensuring that security and compliance with regulations, such as \glsxtrfull{GDPR}, are fully met.

\section{Objectives}
This thesis aims to design and implement a lightweight yet functional architecture for semantic document search in enterprise environments. The proposed system integrates a set of components and services with the following main objectives:
\begin{itemize}
\item \textbf{API Service for Decentralized Semantic Search:} Develop a modular API that enables decentralized semantic search across multiple nodes, eliminating the need for a centralized server while ensuring accessibility and scalability.
\item \textbf{Retrieval-Augmented Generation (RAG) Pipeline:} Implement a complete RAG pipeline capable of processing and indexing documents. This pipeline will extract and store document embeddings and metadata, enabling efficient retrieval of relevant content through semantic queries.
\item \textbf{Metadata Extraction and Organization Suggestions:} Automatically extract metadata from documents to improve search accuracy and suggest organizational structures for better document management.
\item \textbf{Hybrid Search System:} Combine semantic search (using dense vector embeddings) with metadata-based filtering and keyword-based ranking, providing users with more accurate and context-aware results.
\item \textbf{Integration with External LLM and Embedding APIs:} Utilize external models such as OpenAIâ€™s API for embedding generation and language understanding, with mechanisms to log usage and cost, ensuring budget-aware operation.
\item \textbf{Internet-Aware Search Capabilities:} Explore the integration of external sources and databases over the internet to enable the system to retrieve information beyond the local knowledge base, when appropriate.
\end{itemize}
Together, these objectives form the basis of a robust, scalable, and cost-efficient information retrieval system tailored for use in dynamic and distributed enterprise environments.
% #############################################################################
\section{Organization of the Document}
This thesis is is organized as follows: \Cref{chap:intro} 





