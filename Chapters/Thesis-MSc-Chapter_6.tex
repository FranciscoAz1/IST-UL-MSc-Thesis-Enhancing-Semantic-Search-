% #############################################################################
% This is Chapter 6
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Conclusion}
\cleardoublepage%
% The following line allows to ref this chapter
\label{chap:conclusion}%


% #############################################################################
\section{Conclusions}

This thesis examinad strategies to enhance semantic search by integrating retrieval, large language models, structure-aware system design, and agentic workflows. The findings demonstrate that carefully engineered prompting, robust reasoning strategies, and precise grounding in external knowledge souces significantly improves retrieval and answer quality.

This thesis proposes a design for structure-aware systems that exploit an organization's inherent information structure—including cross-references, collection descriptions, and metadata—to inform and constrain retrieval processes. Although large-scale empirical evaluation was beyond the project's scope, the analysis indicates that, within enterprise environments, structural signals can meaningfully enhance retrieval precision and interpretability.

Prompt-engineering strategies for automated document analysis were also explored (e.g., metadata extraction, organization structuring suggestions, targeted entity extraction, and summarization). Although initial results were modest due to computational and budget constraints, the approach remains promising and is likely to improve through more efficient optimization, stronger models, and refined tuning or few-shot prompting aligned with the application-specific schema.

\gls{AAI} systems—incorporating planning, retrieval, observation, and reflection—were compared to a naive retrieval-augmented generation \gls{RAG} baseline. Across the evaluated tasks, the agentic approach consistently produced more accurate and contextually answers, highlighting the importance of reasoning frameworks and prompt strategies in making information more accessible and interpretable.

From an engineering perspective, extensive use was made of the Weaviate vector database, which enabled rapid experimentation with configurations and providing a clear path to scalability, reproducibility, and integration of \glspl{ACL} within corporate environments. An \gls{MCP} server was also built to interface with Weaviate, enabling standard \gls{MCP} clients (e.g., agents and chatbots) to query and traverse the knowledge base (including traversal).

Collectively, this thesis advances \gls{RAG} pipeline techniques for extracting structured information from unstructured documents via systematic prompt engineering. It proposes a structure-aware retrieval design that leverages organizational signals, and delivers an \gls{MCP} server that integrates with a scalable, AI-enabled vector database (Weaviate) to interface with chatbots and tool-using agents—together providing a practical foundation for building more effective and user-friendly information-retrieval systems across domains. Taken together, the results in Chapter~\ref{chap:results} crystallize best practices and configuration guidelines for the schema-aware agent over cross-references in Weaviate—when to favor agentic planning, how to configure hybrid search and top-$k$ parameters, when to enable multi-collection search, and when to apply extractive summarization—so practitioners can deploy the MCP–Weaviate stack in a robust, cost-aware, and accuracy-oriented manner.
% #############################################################################
\section{System Limitations and Future Work}
The primary limitations of this work were computational capacity, financial budget, and data availability. Our local compute was sufficient to run only small open \glspl{LLM}; evaluating stronger models with extended context windows required using hosted APIs. Per-token pricing for hosted \glspl{LLM} (e.g., OpenAI, Google Gemini) kept day-to-day iteration affordable but rendered large-scale ablations and benchmarking prohibitively expensive, which in turn limited the breadth of experiments, the size and complexity of models we could test, and the context-window lengths we could routinely exercise. These constraints also affected the evaluation of document-analysis pipelines: they were comparatively expensive to run and yielded only modest gains in our setting. Finally, the lack of a publicly available dataset tailored to structure-aware retrieval prevented a comprehensive end-to-end assessment, and the specific model choices restrict the generality of our conclusions.

By design, this thesis does not introduce new \gls{ML} or \gls{LLM} architectures. It assumes that contemporary \glspl{LLM} and embedding models are already sufficiently capable for many enterprise applications and will continue to improve primarily through scaling—both in parameter count and context-window lengh. Accordingly, the focus on \gls{RAG} and prompt-engineering strategies that leverage long-context models to produce more accurate and contextually answers—especially for non-expert users who may lack experience in effective prompting \glspl{LLM}.

Future work should:
\begin{itemize}
	\item evaluate stronger base and instruction-tuned models, with extended context windows, on larger and more diverse datasets, to assess scalability and robustness across domains;
	\item develop and release a public benchmark for structure-aware retrieval that captures cross-references, hierarchies, and organizational context, complemented by human-annotated relevance judgments;
	\item systematically compare prompting and reasoning strategies 
	
	(e.g., self-consistency, tree-of-thought, \textsc{ReAct}) across domains and tasks;
	\item integrate additional external knowledge sources and more robust grounding (e.g., entity linking, schema alignment, knowledge graphs, retrieval-augmented planning), and evaluate hallucination/faithfulness;
	\item study user experience among non-experts via controlled studies, measuring usability, transparency, latency, cost, and confidence calibration;
	\item quantify cost–quality trade-offs and throughput under realistic workloads, including caching, reranking, and distillation for efficiency.
\end{itemize}

These directions would enable a more comprehensive assessment of the architecture and its benefits in real-world deployments.



